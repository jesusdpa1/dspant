{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udea7\u26a0 Site under construction!!","text":""},{"location":"#dspant-digital-signal-processing-for-neural-data","title":"dspant: Digital Signal Processing for Neural Data","text":"<p>dspant is a comprehensive Python library for digital signal processing with a focus on neural data analysis. It provides a modular and highly optimized set of tools for loading, processing, analyzing, and visualizing neural time-series data with a strong emphasis on spike detection and sorting.</p> <p>\ud83d\udc1c</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Efficient data loading and conversion: Support for common neural data formats including TDT (Tucker-Davis Technologies)</li> <li>Scalable processing pipelines: Processing large datasets efficiently with Dask arrays and Numba acceleration</li> <li>Comprehensive spike detection and sorting: Multiple algorithms for detecting and classifying neural action potentials</li> <li>Advanced visualization: Rich plotting capabilities for neural data, spike waveforms, and sorting results</li> <li>Quality metrics: Tools for assessing the quality of spike sorting results</li> <li>Modular architecture: Easily extend functionality with custom processors and analysis methods</li> </ul>"},{"location":"#example","title":"Example","text":"<pre><code>from dspant.nodes import StreamNode\nfrom dspant.engine import create_processing_node\nfrom dspant.neuroproc.detection import create_negative_peak_detector\nfrom dspant.neuroproc.extraction import extract_spike_waveforms\n\n# Load neural data\nnode = StreamNode(data_path=\"my_recording\").load_data()\n\n# Create processing pipeline with a spike detector\nproc_node = create_processing_node(node)\ndetector = create_negative_peak_detector(threshold=4.0)\nproc_node.add_processor(detector, group=\"detection\")\n\n# Run the detection\nspikes_df = proc_node.process()\n\n# Extract spike waveforms\nwaveforms, spike_times, metadata = extract_spike_waveforms(\n    node.data, spikes_df, pre_samples=10, post_samples=30\n)\n\n# Visualize results\nfrom dspant.neuroproc.visualization import plot_spike_events\nfig_waveforms, fig_raster = plot_spike_events(\n    node.data, spikes_df, node.fs, time_window=(10, 15)\n)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"core-concepts/data-nodes/","title":"Data Nodes","text":"<p>Data nodes are a core concept in dspant and serve as the primary interface for accessing and manipulating different types of neural data. They handle data loading, validation, metadata management, and provide a consistent API for working with various data types.</p>"},{"location":"core-concepts/data-nodes/#node-types","title":"Node Types","text":"<p>dspant provides several node types for different kinds of data:</p> <ul> <li>BaseNode: The foundation class that provides common functionality for all node types</li> <li>StreamNode: For handling time-series data (continuous recordings)</li> <li>EpocNode: For handling event-based data (epochs or timestamps)</li> </ul>"},{"location":"core-concepts/data-nodes/#base-node","title":"Base Node","text":"<p>All node types inherit from <code>BaseNode</code>, which provides:</p> <ul> <li>Path management for data and metadata files</li> <li>File validation and discovery</li> <li>Metadata loading and parsing</li> </ul>"},{"location":"core-concepts/data-nodes/#stream-node","title":"Stream Node","text":"<p>The <code>StreamNode</code> class is designed for working with continuous time-series data like neural recordings. It provides:</p> <ul> <li>Efficient loading of large datasets using Dask arrays</li> <li>Automatic chunking for memory-efficient processing</li> <li>Sampling rate and channel information management</li> <li>Data shape and type validation</li> </ul>"},{"location":"core-concepts/data-nodes/#epoch-node","title":"Epoch Node","text":"<p>The <code>EpocNode</code> class is specialized for handling event-based data like spike timestamps, stimulus events, or behavioral markers. It provides:</p> <ul> <li>Loading event data into Polars DataFrames for efficient manipulation</li> <li>Event timing and relationship analysis</li> <li>Integration with time-series data for event-locked analyses</li> </ul>"},{"location":"core-concepts/data-nodes/#example-usage","title":"Example Usage","text":""},{"location":"core-concepts/data-nodes/#working-with-stream-nodes","title":"Working with Stream Nodes","text":"<pre><code>from dspant.nodes import StreamNode\n\n# Create a stream node from a data path\nstream_node = StreamNode(data_path=\"path/to/recording.ant\")\n\n# Load the data (lazy loading with Dask)\ndata = stream_node.load_data()\n\n# Access node properties\nprint(f\"Sampling rate: {stream_node.fs} Hz\")\nprint(f\"Duration: {stream_node.number_of_samples / stream_node.fs:.2f} seconds\")\nprint(f\"Channels: {stream_node.channel_numbers}\")\n\n# Print a summary of the node\nstream_node.summarize()\n</code></pre>"},{"location":"core-concepts/data-nodes/#working-with-epoch-nodes","title":"Working with Epoch Nodes","text":"<pre><code>from dspant.nodes import EpocNode\n\n# Create an epoch node from a data path\nepoc_node = EpocNode(data_path=\"path/to/events.ant\")\n\n# Load the event data\nevents = epoc_node.load_data()\n\n# Basic operations with events\nevent_count = len(events)\nfirst_event_time = events[0, \"onset\"]\nevent_intervals = events.with_columns(\n    (pl.col(\"onset\").shift(-1) - pl.col(\"onset\")).alias(\"interval\")\n)\n\n# Print a summary of the node\nepoc_node.summarize()\n</code></pre>"},{"location":"core-concepts/data-nodes/#under-the-hood","title":"Under the Hood","text":"<p>Data nodes use a consistent file structure:</p> <ul> <li>Each node corresponds to a directory with an <code>.ant</code> extension</li> <li>Inside that directory are <code>data_*.parquet</code> files containing the actual data</li> <li>Accompanying <code>metadata_*.json</code> files contain metadata like sampling rate, channel info, etc.</li> </ul> <p>This structure enables efficient data sharing, versioning, and access patterns for different analysis needs.</p>"},{"location":"core-concepts/data-nodes/#integration-with-processing-pipeline","title":"Integration with Processing Pipeline","text":"<p>Data nodes are designed to integrate seamlessly with the processing pipeline components:</p> <pre><code>from dspant.nodes import StreamNode\nfrom dspant.engine import StreamProcessingNode\n\n# Create a stream node and load data\nstream_node = StreamNode(data_path=\"path/to/recording.ant\").load_data()\n\n# Create a processing node that references the stream node\nproc_node = StreamProcessingNode(stream_node)\n\n# Now the processing node can apply operations to the stream node's data\nfiltered_data = proc_node.process()\n</code></pre> <p>This separation of data nodes and processing nodes creates a clean architecture where:</p> <ol> <li>Data nodes focus on data access, validation, and metadata management</li> <li>Processing nodes focus on applying algorithms and transformations to the data</li> </ol>"},{"location":"core-concepts/processing-pipeline/","title":"Processing Pipeline","text":"<p>The processing pipeline is a central concept in dspant that provides a flexible framework for building and executing complex signal processing workflows. It allows you to chain together multiple processing steps, manage their execution, and optimize performance for large datasets.</p>"},{"location":"core-concepts/processing-pipeline/#architecture","title":"Architecture","text":"<p>The processing pipeline architecture consists of three main components:</p> <ol> <li>BaseProcessor: Abstract interface that all processors must implement</li> <li>StreamProcessingPipeline: Manages sequences of processors and their execution</li> <li>StreamProcessingNode: Connects a data node to a processing pipeline</li> </ol> <p>This design provides a clean separation between: - Data access (handled by nodes) - Processing algorithm implementation (handled by processors) - Processing workflow management (handled by pipelines)</p>"},{"location":"core-concepts/processing-pipeline/#baseprocessor","title":"BaseProcessor","text":"<p>The <code>BaseProcessor</code> abstract class defines the interface for all signal processing components:</p> <pre><code>class BaseProcessor(ABC):\n    @abstractmethod\n    def process(self, data: da.Array, fs: Optional[float] = None, **kwargs) -&gt; da.Array:\n        \"\"\"Process the input data\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def overlap_samples(self) -&gt; int:\n        \"\"\"Return the number of samples needed for overlap\"\"\"\n        pass\n</code></pre> <p>All processors must implement: - A <code>process</code> method that takes a Dask array and returns a processed Dask array - An <code>overlap_samples</code> property that defines how many samples are needed at chunk boundaries</p>"},{"location":"core-concepts/processing-pipeline/#streamprocessingpipeline","title":"StreamProcessingPipeline","text":"<p>The <code>StreamProcessingPipeline</code> class manages a sequence of processor instances:</p> <pre><code>pipeline = StreamProcessingPipeline()\n\n# Add processors to different groups\npipeline.add_processor(filter_processor, group=\"filters\")\npipeline.add_processor(envelope_processor, group=\"features\")\npipeline.add_processor(threshold_processor, group=\"detection\")\n\n# Process data with all processors in sequence\nresult = pipeline.process(data, fs=sampling_rate)\n\n# Or process with specific groups\nresult = pipeline.process(\n    data, \n    processors=pipeline.get_group_processors(\"filters\"),\n    fs=sampling_rate\n)\n</code></pre> <p>Key features include: - Organizing processors into named groups - Adding, removing, or replacing processors dynamically - Processing data with specific processor groups or sequences</p>"},{"location":"core-concepts/processing-pipeline/#streamprocessingnode","title":"StreamProcessingNode","text":"<p>The <code>StreamProcessingNode</code> connects a data node to a processing pipeline:</p> <pre><code>from dspant.nodes import StreamNode\nfrom dspant.engine import StreamProcessingNode, create_pipeline\n\n# Create a data node\nstream_node = StreamNode(data_path=\"path/to/recording.ant\").load_data()\n\n# Create a processing node connected to the data node\nproc_node = StreamProcessingNode(stream_node, name=\"my_processing\")\n\n# Add processors to the pipeline\nproc_node.add_processor(filter_processor, group=\"filters\")\nproc_node.add_processor([detector1, detector2], group=\"detection\")\n\n# Process the data\nresult = proc_node.process(group=[\"filters\", \"detection\"])\n</code></pre> <p>Key features include: - Direct connection to a data node - Managing processor execution and data flow - Tracking processing history and state - Handling memory optimization and chunk management</p>"},{"location":"core-concepts/processing-pipeline/#performance-optimization","title":"Performance Optimization","text":"<p>The processing pipeline includes several features for optimizing performance with large datasets:</p>"},{"location":"core-concepts/processing-pipeline/#chunk-optimization","title":"Chunk Optimization","text":"<pre><code># Process with chunk optimization\nresult = proc_node.process(\n    optimize_chunks=True,\n    persist_intermediates=True,\n    num_workers=4\n)\n</code></pre> <p>Chunk optimization features include: - Automatic adjustment of chunk sizes based on processor requirements - Persisting intermediate results for faster recomputation - Controlling the number of parallel workers</p>"},{"location":"core-concepts/processing-pipeline/#processor-overlap","title":"Processor Overlap","text":"<p>The pipeline handles the complexity of ensuring that chunk boundaries have sufficient context for accurate processing, using the <code>overlap_samples</code> property from each processor.</p>"},{"location":"core-concepts/processing-pipeline/#stream-processing","title":"Stream Processing","text":"<p>Rather than loading all data into memory, the pipeline processes data in streaming chunks, enabling the analysis of very large datasets that wouldn't fit in memory.</p>"},{"location":"core-concepts/processing-pipeline/#creating-custom-processors","title":"Creating Custom Processors","text":"<p>You can create custom processors by implementing the <code>BaseProcessor</code> interface:</p> <pre><code>from dspant.engine import BaseProcessor\nimport dask.array as da\nimport numpy as np\n\nclass CustomFilter(BaseProcessor):\n    def __init__(self, cutoff_hz):\n        self.cutoff_hz = cutoff_hz\n        self._overlap_samples = 100  # Samples needed at chunk boundaries\n\n    def process(self, data: da.Array, fs: float = None, **kwargs) -&gt; da.Array:\n        \"\"\"Apply custom filtering to the input data\"\"\"\n        # Implement your processing logic here\n        # This is typically a map of a numpy function over dask chunks\n        return data.map_blocks(\n            self._filter_chunk, \n            fs=fs, \n            cutoff=self.cutoff_hz,\n            dtype=data.dtype\n        )\n\n    def _filter_chunk(self, chunk, fs, cutoff):\n        # Process a single chunk using numpy\n        # Add your NumPy-based filtering code here\n        return filtered_chunk\n\n    @property\n    def overlap_samples(self) -&gt; int:\n        return self._overlap_samples\n\n    @property\n    def summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Return processor configuration details\"\"\"\n        return {\n            \"type\": \"CustomFilter\",\n            \"cutoff_hz\": self.cutoff_hz,\n            \"overlap\": self._overlap_samples\n        }\n</code></pre>"},{"location":"core-concepts/processing-pipeline/#factory-functions","title":"Factory Functions","text":"<p>For common processor combinations, dspant provides factory functions:</p> <pre><code>from dspant.engine import create_pipeline\nfrom dspant.processor.filters import create_bandpass_filter\nfrom dspant.processor.feature import create_envelope_extractor\n\n# Create common processors\npipeline = create_pipeline()\nbandpass = create_bandpass_filter(low_hz=300, high_hz=3000)\nenvelope = create_envelope_extractor(smoothing_ms=5)\n\n# Add to pipeline\npipeline.add_processor([bandpass, envelope], group=\"preprocessing\")\n</code></pre> <p>These factory functions provide a simpler interface for creating commonly used processor configurations.</p>"},{"location":"emg-processor/overview/","title":"EMG Processing Overview","text":"<p>The EMG processing module provides specialized tools for analyzing and processing electromyography (EMG) signals. This module is designed to handle the unique characteristics of EMG data, including noise filtering, feature extraction, and analysis of muscle activation patterns.</p>"},{"location":"emg-processor/overview/#key-features","title":"Key Features","text":"<ul> <li>Signal preprocessing: Specialized filters for EMG denoising and artifact removal</li> <li>Envelope extraction: Methods for extracting muscle activation envelopes</li> <li>Feature calculation: Time and frequency domain feature extraction</li> <li>Burst detection: Algorithms for detecting muscle activation bursts</li> <li>Fatigue analysis: Tools for monitoring and analyzing muscle fatigue</li> <li>Visualization: Specialized plots for EMG signal visualization</li> </ul>"},{"location":"emg-processor/overview/#module-structure","title":"Module Structure","text":"<p>The EMG processing module is organized into several submodules:</p> <ul> <li>Preprocessing: Signal filtering and conditioning</li> <li>Features: Feature extraction in time and frequency domains</li> <li>Detection: Muscle activation detection algorithms</li> <li>Analysis: Advanced analysis tools for muscle activation patterns</li> <li>Visualization: Specialized plotting functions for EMG data</li> </ul>"},{"location":"emg-processor/overview/#signal-preprocessing","title":"Signal Preprocessing","text":"<p>EMG signals often require specialized preprocessing to remove artifacts and noise:</p> <pre><code>from dspant.processor.filters import create_bandpass_filter\nfrom dspant.processor.emg import create_emg_envelope_extractor\n\n# Create EMG-specific filters\nbandpass = create_bandpass_filter(\n    low_hz=20,    # High-pass to remove motion artifacts\n    high_hz=500,  # Low-pass to remove high-frequency noise\n    order=4\n)\n\n# Create envelope extractor\nenvelope = create_emg_envelope_extractor(\n    rectify=True,           # Full-wave rectification\n    smoothing_ms=100,       # 100ms smoothing window\n    method=\"rms\"            # Root Mean Square method\n)\n</code></pre>"},{"location":"emg-processor/overview/#feature-extraction","title":"Feature Extraction","text":"<p>The module provides tools for extracting common EMG features:</p> <pre><code>from dspant.processor.emg.features import (\n    extract_amplitude_features,\n    extract_frequency_features,\n    extract_complexity_features\n)\n\n# Extract time-domain features\ntime_features = extract_amplitude_features(\n    emg_data,\n    features=[\"rms\", \"iemg\", \"mav\", \"wl\", \"zc\", \"ssc\"],\n    window_ms=250,\n    overlap_ms=125\n)\n\n# Extract frequency-domain features\nfreq_features = extract_frequency_features(\n    emg_data,\n    features=[\"mnf\", \"mdf\", \"pkf\", \"mnp\"],\n    fs=1000,  # 1kHz sampling rate\n    window_ms=250\n)\n</code></pre>"},{"location":"emg-processor/overview/#muscle-activity-detection","title":"Muscle Activity Detection","text":"<p>Detect muscle activation bursts in EMG signals:</p> <pre><code>from dspant.processor.emg.detection import create_muscle_activity_detector\n\n# Create activity detector\ndetector = create_muscle_activity_detector(\n    threshold_method=\"dynamic_rms\",  # Dynamic RMS-based threshold\n    threshold_factor=3.0,           # 3x RMS\n    min_duration_ms=50,             # Minimum 50ms activation\n    merge_distance_ms=25            # Merge activations within 25ms\n)\n\n# Apply detector\nactivations = detector.process(envelope_data, fs=1000)\n</code></pre>"},{"location":"emg-processor/overview/#fatigue-analysis","title":"Fatigue Analysis","text":"<p>Tools for monitoring muscle fatigue during sustained contractions:</p> <pre><code>from dspant.processor.emg.analysis import compute_fatigue_metrics\n\n# Compute fatigue metrics\nfatigue_metrics = compute_fatigue_metrics(\n    emg_data,\n    fs=1000,\n    window_s=1.0,  # 1-second windows\n    features=[\"mnf\", \"mdf\", \"rms_slope\"]\n)\n</code></pre>"},{"location":"emg-processor/overview/#emg-specific-processing-pipeline","title":"EMG-Specific Processing Pipeline","text":"<p>A complete EMG processing pipeline combining multiple steps:</p> <pre><code>from dspant.nodes import StreamNode\nfrom dspant.engine import create_processing_node\nfrom dspant.processor.emg import create_emg_processing_pipeline\n\n# Load EMG data\nemg_node = StreamNode(\"path/to/emg_data.ant\").load_data()\n\n# Create processing node\nproc_node = create_processing_node(emg_node)\n\n# Create and add a complete EMG processing pipeline\nemg_pipeline = create_emg_processing_pipeline(\n    bandpass_range=(20, 500),     # Bandpass filter range\n    notch_freq=50,                # Power line frequency\n    envelope_smoothing_ms=100,    # Envelope smoothing window\n    envelope_method=\"rms\"         # RMS envelope method\n)\nproc_node.add_processor(emg_pipeline, group=\"emg_processing\")\n\n# Process data\nprocessed_emg = proc_node.process()\n</code></pre>"},{"location":"emg-processor/overview/#visualization-functions","title":"Visualization Functions","text":"<p>Specialized plotting functions for EMG data:</p> <pre><code>from dspant.processor.emg.visualization import (\n    plot_emg_with_envelope,\n    plot_muscle_activations,\n    plot_frequency_spectrum,\n    plot_fatigue_progression\n)\n\n# Plot EMG signal with envelope\nfig1 = plot_emg_with_envelope(\n    raw_emg=emg_node.data,\n    envelope=processed_emg,\n    fs=emg_node.fs,\n    time_window=(10, 15)  # 5-second window\n)\n\n# Plot detected muscle activations\nfig2 = plot_muscle_activations(\n    emg_data=emg_node.data,\n    activations=activations,\n    fs=emg_node.fs\n)\n\n# Plot frequency content changes over time\nfig3 = plot_frequency_spectrum(\n    emg_data=emg_node.data,\n    fs=emg_node.fs,\n    window_s=0.5,\n    overlap=0.25\n)\n</code></pre>"},{"location":"emg-processor/overview/#integration-with-other-modules","title":"Integration with Other Modules","text":"<p>The EMG processing module integrates with other dspant components:</p> <ul> <li>Works with the StreamNode system for efficient data handling</li> <li>Leverages the Processing Pipeline for flexible workflow construction</li> <li>Can be combined with General Signal Processing tools for custom analyses</li> <li>Outputs compatible with Visualization tools for publication-ready figures</li> </ul>"},{"location":"general-processor/overview/","title":"General Signal Processing Overview","text":"<p>The general processing module provides a comprehensive set of tools for digital signal processing across various signal types. These processors implement common signal processing operations and can be used as building blocks for more specialized processing pipelines.</p>"},{"location":"general-processor/overview/#key-features","title":"Key Features","text":"<ul> <li>Filtering: Various filter types including FIR, IIR, adaptive, and non-linear filters</li> <li>Transforms: Fourier, wavelet, and Hilbert transforms for frequency and time-frequency analysis</li> <li>Feature extraction: Amplitude, frequency, and statistical feature computation</li> <li>Signal conditioning: Normalization, baseline correction, and artifact removal</li> <li>Dimensionality reduction: PCA, ICA, and other techniques for reducing data complexity</li> <li>Segmentation: Windowing, epoch extraction, and continuous signal segmentation</li> <li>Resampling: Up-sampling, down-sampling, and interpolation methods</li> </ul>"},{"location":"general-processor/overview/#module-structure","title":"Module Structure","text":"<p>The general processing module is organized into several submodules:</p> <ul> <li>Filters: Signal filtering components</li> <li>Transforms: Signal transformation operations</li> <li>Features: Feature extraction processors</li> <li>Conditioning: Signal conditioning processors</li> <li>Dimensionality: Dimensionality reduction techniques</li> <li>Segmentation: Signal segmentation tools</li> <li>Resampling: Sample rate conversion methods</li> </ul>"},{"location":"general-processor/overview/#filtering-operations","title":"Filtering Operations","text":"<p>Filter processors provide various frequency-selective operations:</p> <pre><code>from dspant.processor.filters import (\n    create_bandpass_filter,\n    create_lowpass_filter,\n    create_highpass_filter,\n    create_bandstop_filter,\n    create_notch_filter\n)\n\n# Create a bandpass filter (300-3000 Hz)\nbandpass = create_bandpass_filter(\n    low_hz=300,\n    high_hz=3000,\n    order=4,\n    filter_type=\"butterworth\"\n)\n\n# Create a lowpass filter (cutoff at 100 Hz)\nlowpass = create_lowpass_filter(\n    cutoff_hz=100,\n    order=6,\n    filter_type=\"bessel\"\n)\n\n# Create a 50 Hz notch filter (for power line noise)\nnotch = create_notch_filter(\n    center_hz=50,\n    q_factor=30\n)\n</code></pre>"},{"location":"general-processor/overview/#transform-operations","title":"Transform Operations","text":"<p>Transform processors convert signals between domains:</p> <pre><code>from dspant.processor.transforms import (\n    create_fft_processor,\n    create_wavelet_processor,\n    create_hilbert_processor\n)\n\n# FFT processor for spectral analysis\nfft_proc = create_fft_processor(\n    window=\"hann\",\n    nfft=1024,\n    return_type=\"magnitude\"\n)\n\n# Wavelet transform for time-frequency analysis\nwavelet_proc = create_wavelet_processor(\n    wavelet=\"morlet\",\n    scales=np.arange(1, 128),\n    output=\"power\"\n)\n\n# Hilbert transform for envelope and phase analysis\nhilbert_proc = create_hilbert_processor(\n    output=\"envelope\"  # Options: envelope, phase, complex\n)\n</code></pre>"},{"location":"general-processor/overview/#feature-extraction","title":"Feature Extraction","text":"<p>Feature extraction processors compute signal characteristics:</p> <pre><code>from dspant.processor.features import (\n    create_amplitude_feature_extractor,\n    create_frequency_feature_extractor,\n    create_statistical_feature_extractor\n)\n\n# Extract amplitude features\namp_features = create_amplitude_feature_extractor(\n    features=[\"rms\", \"peak\", \"peak_to_peak\", \"mean_abs\"],\n    window_ms=500,\n    step_ms=250\n)\n\n# Extract frequency features\nfreq_features = create_frequency_feature_extractor(\n    features=[\"mean_freq\", \"median_freq\", \"band_power\"],\n    window_ms=1000,\n    bands=[(0.5, 4), (4, 8), (8, 13), (13, 30)]  # Standard EEG bands\n)\n\n# Extract statistical features\nstat_features = create_statistical_feature_extractor(\n    features=[\"mean\", \"std\", \"skewness\", \"kurtosis\", \"percentile\"],\n    percentiles=[5, 50, 95]\n)\n</code></pre>"},{"location":"general-processor/overview/#signal-conditioning","title":"Signal Conditioning","text":"<p>Signal conditioning processors prepare signals for analysis:</p> <pre><code>from dspant.processor.conditioning import (\n    create_normalizer,\n    create_baseline_corrector,\n    create_artifact_remover\n)\n\n# Normalize signal to z-scores\nnormalizer = create_normalizer(\n    method=\"zscore\",  # Options: zscore, minmax, robust\n    axis=0\n)\n\n# Correct baseline using polynomial fitting\nbaseline_corrector = create_baseline_corrector(\n    method=\"polynomial\",\n    polynomial_order=3,\n    baseline_region=(0, 1000)  # First 1000 samples\n)\n\n# Remove artifacts using threshold-based approach\nartifact_remover = create_artifact_remover(\n    method=\"threshold\",\n    threshold=5.0,  # 5 standard deviations\n    window_ms=200\n)\n</code></pre>"},{"location":"general-processor/overview/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction processors simplify complex signals:</p> <pre><code>from dspant.processor.dimensionality import (\n    create_pca_processor,\n    create_ica_processor\n)\n\n# PCA dimensionality reduction\npca_proc = create_pca_processor(\n    n_components=10,\n    whiten=True\n)\n\n# ICA for source separation\nica_proc = create_ica_processor(\n    n_components=5,\n    algorithm=\"extended-infomax\"\n)\n</code></pre>"},{"location":"general-processor/overview/#segmentation","title":"Segmentation","text":"<p>Segmentation processors divide continuous signals into chunks:</p> <pre><code>from dspant.processor.segmentation import (\n    create_windowing_processor,\n    create_event_segmenter\n)\n\n# Window the signal into overlapping segments\nwindowing = create_windowing_processor(\n    window_ms=1000,\n    step_ms=500,\n    window_type=\"hamming\"\n)\n\n# Extract segments around events\nevent_segmenter = create_event_segmenter(\n    pre_event_ms=200,\n    post_event_ms=800,\n    baseline_ms=100\n)\n</code></pre>"},{"location":"general-processor/overview/#resampling","title":"Resampling","text":"<p>Resampling processors change the sampling rate:</p> <pre><code>from dspant.processor.resampling import (\n    create_downsampler,\n    create_upsampler,\n    create_resampler\n)\n\n# Downsample by a factor\ndownsampler = create_downsampler(\n    factor=4,\n    filter_type=\"fir\",\n    filter_order=64\n)\n\n# Upsample by a factor\nupsampler = create_upsampler(\n    factor=2,\n    filter_type=\"fir\",\n    filter_order=32\n)\n\n# Resample to specific sampling rate\nresampler = create_resampler(\n    output_fs=1000,  # Target sampling rate\n    method=\"sinc\"    # Options: sinc, linear, cubic\n)\n</code></pre>"},{"location":"general-processor/overview/#building-processing-pipelines","title":"Building Processing Pipelines","text":"<p>General processors can be combined into processing pipelines:</p> <pre><code>from dspant.nodes import StreamNode\nfrom dspant.engine import create_processing_node\n\n# Load data\nstream_node = StreamNode(\"path/to/data.ant\").load_data()\nproc_node = create_processing_node(stream_node)\n\n# Create a sequence of processors\nproc_node.add_processor(notch, group=\"preprocessing\")\nproc_node.add_processor(bandpass, group=\"preprocessing\")\nproc_node.add_processor(normalizer, group=\"preprocessing\")\nproc_node.add_processor(amp_features, group=\"feature_extraction\")\n\n# Process data through the pipeline\nresults = proc_node.process()\n</code></pre>"},{"location":"general-processor/overview/#processor-customization","title":"Processor Customization","text":"<p>All processors can be customized beyond the factory functions:</p> <pre><code>from dspant.processor.filters import ButterworthFilter\nfrom dspant.engine.base import BaseProcessor\n\n# Customize a filter directly\ncustom_filter = ButterworthFilter(\n    cutoff_hz=(300, 3000),\n    filter_type=\"bandpass\",\n    order=4,\n    zero_phase=True,\n    padtype=\"constant\"\n)\n\n# Create a custom processor by subclassing BaseProcessor\nclass CustomProcessor(BaseProcessor):\n    def __init__(self, parameter1, parameter2):\n        self.parameter1 = parameter1\n        self.parameter2 = parameter2\n        self._overlap_samples = 100\n\n    def process(self, data, fs=None, **kwargs):\n        # Implement custom processing here\n        return processed_data\n\n    @property\n    def overlap_samples(self):\n        return self._overlap_samples\n\n    @property\n    def summary(self):\n        return {\n            \"type\": \"CustomProcessor\",\n            \"parameter1\": self.parameter1,\n            \"parameter2\": self.parameter2,\n            \"overlap\": self._overlap_samples\n        }\n</code></pre>"},{"location":"general-processor/overview/#optimization-techniques","title":"Optimization Techniques","text":"<p>General processors include performance optimization features:</p> <pre><code># Create a processor with Numba acceleration\nfrom dspant.processor.features import create_statistical_feature_extractor\n\n# Numba-accelerated feature extractor\nfast_features = create_statistical_feature_extractor(\n    features=[\"mean\", \"std\", \"percentile\"],\n    percentiles=[5, 50, 95],\n    use_numba=True  # Enable Numba acceleration\n)\n\n# Process with optimized chunk handling\nresults = proc_node.process(\n    optimize_chunks=True,       # Automatically optimize chunk sizes\n    persist_intermediates=True, # Store intermediate results\n    num_workers=4               # Use 4 workers for parallel processing\n)\n</code></pre>"},{"location":"general-processor/overview/#common-signal-processing-workflows","title":"Common Signal Processing Workflows","text":"<p>The general processing module supports several common workflows:</p> <ol> <li>Signal cleaning: Filter out noise and artifacts</li> <li>Feature engineering: Extract informative features for machine learning</li> <li>Time-frequency analysis: Analyze signal properties across time and frequency</li> <li>Event detection: Identify important events in continuous signals</li> <li>Data reduction: Compress and simplify complex signals</li> </ol>"},{"location":"general-processor/overview/#integration-with-specialized-modules","title":"Integration with Specialized Modules","text":"<p>General processors are building blocks for specialized modules:</p> <ul> <li>Neural processing: Specialized processors built on general processing concepts</li> <li>EMG processing: Muscle activity analysis using general signal processing foundations</li> <li>EEG processing: Brain activity analysis with signal processing fundamentals</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers how to install dspant and its dependencies for different use cases.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>dspant requires Python 3.8 or higher and depends on several scientific computing libraries:</p> <ul> <li>NumPy (&gt;=1.20.0)</li> <li>SciPy (&gt;=1.7.0)</li> <li>Dask (&gt;=2022.2.0)</li> <li>Pandas (&gt;=1.3.0)</li> <li>Polars (&gt;=0.15.0)</li> <li>PyArrow (&gt;=7.0.0)</li> <li>Matplotlib (&gt;=3.4.0)</li> <li>Numba (&gt;=0.54.0)</li> </ul>"},{"location":"getting-started/installation/#current-installation","title":"Current Installation","text":"<p>For current installation </p> <pre><code>git clone #repo\ncd # to location\n# if uv available\nuv sync\n# else after creating a venv\npip install -e \n</code></pre>"},{"location":"getting-started/installation/#checking-your-installation","title":"Checking Your Installation","text":"<p>To verify that dspant has been installed correctly:</p> <pre><code>import dspant\nprint(dspant.__version__)\n\n# Test loading some basic modules\nfrom dspant.nodes import StreamNode\nfrom dspant.engine import create_processing_node\nfrom dspant.neuroproc.detection import create_negative_peak_detector\n\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have dspant installed, proceed to the Quickstart Guide to learn how to use the library.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>This guide will walk you through the basics of using dspant for neural data analysis, including loading data, building processing pipelines, detecting spikes, and visualizing results.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<p>First, make sure you have dspant installed:</p> <pre><code>pip install dspant\n</code></pre> <p>For development or the latest features, you can install from the repository:</p> <pre><code>git clone https://github.com/yourusername/dspant.git\ncd dspant\npip install -e .\n</code></pre>"},{"location":"getting-started/quickstart/#loading-data","title":"Loading Data","text":"<p>dspant uses data nodes to load and manage different types of neural data. Let's start by loading some time-series data:</p> <pre><code>from dspant.nodes import StreamNode\n\n# Load from existing ANT format data\nstream_node = StreamNode(data_path=\"path/to/recording.ant\")\nstream_node.load_data()  # Loads the data lazily\n\n# Print a summary of the data\nstream_node.summarize()\n</code></pre> <p>If you have data in TDT format, you can convert it to ANT format first:</p> <pre><code>from dspant.io import convert_tdt_to_ant\n\n# Convert TDT block to ANT format\nant_path = convert_tdt_to_ant(\n    tdt_block_path=\"path/to/tdt_block\",\n    stream_names=[\"Wave\"]  # Which streams to convert\n)\n\n# Now load the converted data\nstream_node = StreamNode(data_path=f\"{ant_path}/Wave.ant\")\nstream_node.load_data()\n</code></pre>"},{"location":"getting-started/quickstart/#basic-signal-processing","title":"Basic Signal Processing","text":"<p>Let's create a simple processing pipeline to filter the data:</p> <pre><code>from dspant.engine import create_processing_node\nfrom dspant.processor.filters import create_bandpass_filter\nimport matplotlib.pyplot as plt\n\n# Create a processing node connected to our data\nproc_node = create_processing_node(stream_node)\n\n# Create a bandpass filter (300-3000 Hz)\nbandpass = create_bandpass_filter(low_hz=300, high_hz=3000)\n\n# Add the filter to the processing pipeline\nproc_node.add_processor(bandpass, group=\"filtering\")\n\n# Process the data\nfiltered_data = proc_node.process()\n\n# Plot a segment of the original and filtered data\nt = np.arange(10000) / stream_node.fs\nplt.figure(figsize=(12, 6))\nplt.subplot(2, 1, 1)\nplt.plot(t, stream_node.data[:10000, 0].compute())\nplt.title(\"Original Data\")\nplt.subplot(2, 1, 2)\nplt.plot(t, filtered_data[:10000].compute())\nplt.title(\"Filtered Data (300-3000 Hz)\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"getting-started/quickstart/#spike-detection","title":"Spike Detection","text":"<p>Now let's detect spikes in our filtered data:</p> <pre><code>from dspant.neuroproc.detection import create_negative_peak_detector\n\n# Create a spike detector\ndetector = create_negative_peak_detector(\n    threshold=4.0,  # 4x MAD threshold\n    refractory_period=0.001  # 1 ms refractory period\n)\n\n# Clear any existing processors and add our filter and detector\nproc_node.clear_processors()\nproc_node.add_processor(bandpass, group=\"preprocessing\")\nproc_node.add_processor(detector, group=\"detection\")\n\n# Run spike detection\nspike_df = proc_node.process()\n\n# Display spike detection results\nprint(f\"Detected {len(spike_df)} spikes\")\nprint(spike_df.head())\n</code></pre>"},{"location":"getting-started/quickstart/#visualizing-spikes","title":"Visualizing Spikes","text":"<p>Let's visualize the detected spikes:</p> <pre><code>from dspant.neuroproc.visualization import plot_spike_events\n\n# Create visualization\nfig_waveforms, fig_raster = plot_spike_events(\n    stream_node.data,  # Original data\n    spike_df,          # Spike detection results\n    stream_node.fs,    # Sampling rate\n    time_window=(10, 15),  # 5-second window from 10-15 seconds\n    window_ms=2.0      # 2 ms window around each spike\n)\n\n# Display plots\nfig_waveforms.savefig(\"spike_waveforms.png\")\nfig_raster.savefig(\"spike_raster.png\")\n</code></pre>"},{"location":"getting-started/quickstart/#extracting-spike-waveforms","title":"Extracting Spike Waveforms","text":"<p>Now let's extract the waveforms around each spike for further analysis:</p> <pre><code>from dspant.neuroproc.extraction import extract_spike_waveforms\n\n# Extract waveforms around spikes\nwaveforms, spike_times, metadata = extract_spike_waveforms(\n    stream_node.data,  # Raw data\n    spike_df,          # Spike detection results\n    pre_samples=10,    # Samples before spike peak\n    post_samples=30,   # Samples after spike peak\n    align_to_min=True, # Align to negative peak\n    compute_result=True # Compute result immediately\n)\n\nprint(f\"Extracted {len(waveforms)} waveforms\")\nprint(f\"Waveform shape: {waveforms.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#spike-sorting","title":"Spike Sorting","text":"<p>Let's perform PCA-KMeans spike sorting on the extracted waveforms:</p> <pre><code>from dspant.neuroproc.sorters import create_numba_pca_kmeans\nimport matplotlib.pyplot as plt\n\n# Create a PCA-KMeans processor\nsorter = create_numba_pca_kmeans(\n    n_clusters=3,       # Number of clusters to find\n    n_components=10,    # Number of PCA components\n    normalize=True      # Normalize waveforms\n)\n\n# Process the waveforms\ncluster_labels = sorter.process(waveforms)\n\n# Visualize the clustering results\nfig = sorter.plot_clusters(\n    waveforms=waveforms,\n    max_points=5000,     # Maximum points to plot\n    alpha=0.7,           # Transparency of points\n    s=15,                # Size of points\n    plot_waveforms=True  # Show waveform shapes\n)\n\nplt.savefig(\"spike_clusters.png\")\n</code></pre>"},{"location":"getting-started/quickstart/#computing-quality-metrics","title":"Computing Quality Metrics","text":"<p>Finally, let's compute quality metrics for our sorted units:</p> <p>```python from dspant.neuroproc.metrics import compute_quality_metrics from dspant.neuroproc.utils import prepare_spike_data_for_metrics</p>"},{"location":"getting-started/quickstart/#convert-spike-times-to-the-format-expected-by-the-metrics-module","title":"Convert spike times to the format expected by the metrics module","text":"<p>spike_times_by_cluster = {} for cluster in np.unique(cluster_labels):     mask = cluster_labels == cluster     spike_times_by_cluster[f\"unit_{cluster}\"] = [spike_times[mask]]</p>"},{"location":"getting-started/quickstart/#compute-quality-metrics","title":"Compute quality metrics","text":"<p>metrics = compute_quality_metrics(     spike_times=spike_times_by_cluster,     sampling_frequency=stream_node.fs,     total_duration=len(stream_node.data) / stream_node.fs,     metrics=[\"num_spikes\", \"firing_rate\", \"presence_ratio\", \"isi_violation\"] )</p>"},{"location":"getting-started/quickstart/#print-metrics","title":"Print metrics","text":"<p>for metric_name, metric_values in metrics.items():     print(f\"\\n{metric_name}:\")     for unit_id, value in metric_values.items():         print(f\"  {unit_id}: {value}\")</p>"},{"location":"neural-processor/api/","title":"Neural Processing API Reference","text":"<p>This API reference provides detailed documentation for the key classes and functions in the <code>dspant.neuroproc</code> module.</p>"},{"location":"neural-processor/api/#detection-module","title":"Detection Module","text":"<p>Neural spike detection module.</p> <p>This module provides tools for detecting action potentials (spikes) in extracellular neural recordings using various methods.</p>"},{"location":"neural-processor/api/#dspant.neuroproc.detection-classes","title":"Classes","text":""},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector","title":"<code>PeakDetector</code>","text":"<p>               Bases: <code>ThresholdDetector</code></p> <p>Peak detector for neural spikes based on threshold crossing and local extrema.</p> <p>This detector identifies peaks by first applying thresholds and then finding local maxima/minima. It supports configurable polarity, refractory periods, and noise-based adaptive thresholds.</p> Source code in <code>src\\dspant\\neuroproc\\detection\\peak_detector.py</code> <pre><code>class PeakDetector(ThresholdDetector):\n    \"\"\"\n    Peak detector for neural spikes based on threshold crossing and local extrema.\n\n    This detector identifies peaks by first applying thresholds and then\n    finding local maxima/minima. It supports configurable polarity, refractory\n    periods, and noise-based adaptive thresholds.\n    \"\"\"\n\n    def __init__(\n        self,\n        threshold: float = 4.0,\n        threshold_mode: Literal[\"absolute\", \"std\", \"mad\", \"rms\"] = \"mad\",\n        polarity: Literal[\"positive\", \"negative\", \"both\"] = \"negative\",\n        refractory_period: float = 0.001,  # 1ms default refractory period\n        align_to_peak: bool = True,  # Align detection to peak rather than threshold crossing\n        noise_estimation_kwargs: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Initialize the peak detector.\n\n        Args:\n            threshold: Threshold value (interpretation depends on threshold_mode)\n            threshold_mode: How to interpret the threshold\n                \"absolute\": Use raw value\n                \"std\": Multiple of signal standard deviation\n                \"mad\": Multiple of median absolute deviation (more robust)\n                \"rms\": Multiple of root mean square\n            polarity: Which peaks to detect\n                \"positive\": Only detect positive peaks\n                \"negative\": Only detect negative peaks (typical for extracellular spikes)\n                \"both\": Detect both positive and negative peaks\n            refractory_period: Minimum time between spikes in seconds\n            align_to_peak: Whether to align detection to peak rather than threshold crossing\n            noise_estimation_kwargs: Parameters for noise estimation if threshold_mode is \"mad\"\n        \"\"\"\n        super().__init__(\n            threshold=threshold,\n            threshold_mode=threshold_mode,\n            polarity=polarity,\n            refractory_period=refractory_period,\n        )\n\n        self.align_to_peak = align_to_peak\n\n        # Set up noise estimation if needed\n        self.noise_estimation_kwargs = noise_estimation_kwargs or {}\n\n        if self.threshold_mode == \"mad\":\n            self.noise_estimator = NoiseEstimationProcessor(\n                method=\"mad\", **self.noise_estimation_kwargs\n            )\n        else:\n            self.noise_estimator = None\n\n        # Overlap samples will be calculated dynamically based on fs and refractory_period\n        self._overlap_samples = None\n\n        # Initialize detection flags\n        self.detect_positive = polarity in [\"positive\", \"both\"]\n        self.detect_negative = polarity in [\"negative\", \"both\"]\n\n    def detect(self, data: da.Array, fs: float, **kwargs) -&gt; pl.DataFrame:\n        \"\"\"\n        Detect peaks in the input data.\n\n        Args:\n            data: Input dask array\n            fs: Sampling frequency in Hz\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            Polars DataFrame with detection results containing:\n                - index: Sample index of the peak\n                - amplitude: Peak amplitude\n                - channel: Channel number\n                - time_sec: Time of the peak in seconds\n        \"\"\"\n        # Ensure data is 2D\n        if data.ndim == 1:\n            data = data.reshape(-1, 1)\n\n        # Calculate minimum distance in samples\n        min_distance_samples = max(int(self.refractory_period * fs), 1)\n\n        # Set overlap samples based on refractory period and sampling rate\n        # Using 3x refractory period ensures we don't miss peaks at chunk boundaries\n        self._overlap_samples = max(min_distance_samples * 3, 10)\n\n        # Calculate adaptive thresholds if using MAD-based threshold\n        if self.threshold_mode == \"mad\" and self.noise_estimator is not None:\n            noise_levels = self.noise_estimator.process(data, fs=fs)\n\n            # Convert to numpy for easier handling\n            noise_levels = noise_levels.compute()\n\n            # For simplicity, use the mean noise level across channels for threshold\n            mean_noise = float(np.mean(noise_levels))\n            pos_threshold = self.threshold * mean_noise\n            neg_threshold = -self.threshold * mean_noise\n        else:\n            # Use compute_threshold from base class for other threshold types\n            thresholds = self.compute_threshold(data.compute(), fs)\n            # Use the mean threshold across channels\n            pos_threshold = float(np.mean(thresholds[\"positive\"]))\n            neg_threshold = float(np.mean(thresholds[\"negative\"]))\n\n        # Function to apply to each chunk with specified overlap\n        def detect_peaks_chunk(chunk, block_info=None):\n            # Get chunk offset for correct indexing in the full array\n            chunk_offset = 0\n            if (\n                block_info\n                and len(block_info) &gt; 0\n                and len(block_info[0][\"array-location\"]) &gt; 0\n            ):\n                try:\n                    chunk_offset = block_info[0][\"array-location\"][0][0]\n                except (IndexError, KeyError):\n                    chunk_offset = 0\n\n            # Ensure chunk is contiguous and in correct layout\n            chunk = np.ascontiguousarray(chunk)\n\n            # Use numba-accelerated implementation\n            peak_indices, peak_amplitudes, peak_channels = _detect_peaks_numba(\n                chunk,\n                pos_threshold,\n                neg_threshold,\n                self.detect_positive,\n                self.detect_negative,\n                min_distance_samples,\n            )\n\n            # Create empty result with correct structure when no peaks are found\n            if len(peak_indices) == 0:\n                # Return empty array but with the correct structured dtype\n                return np.array(\n                    [],\n                    dtype=[\n                        (\"index\", np.int64),\n                        (\"amplitude\", np.float32),\n                        (\"channel\", np.int32),\n                    ],\n                )\n\n            # Add chunk offset to indices\n            peak_indices = peak_indices + chunk_offset\n\n            # Create structured array for output\n            result = np.zeros(\n                len(peak_indices),\n                dtype=[\n                    (\"index\", np.int64),\n                    (\"amplitude\", np.float32),\n                    (\"channel\", np.int32),\n                ],\n            )\n\n            result[\"index\"] = peak_indices\n            result[\"amplitude\"] = peak_amplitudes\n            result[\"channel\"] = peak_channels\n\n            return result\n\n        # Apply detection to chunks with overlap\n        result = data.map_overlap(\n            detect_peaks_chunk,\n            depth={-2: self._overlap_samples},\n            boundary=\"reflect\",\n            dtype=np.dtype(\n                [\n                    (\"index\", np.int64),\n                    (\"amplitude\", np.float32),\n                    (\"channel\", np.int32),\n                ]\n            ),\n            meta=np.array(\n                [],\n                dtype=np.dtype(\n                    [\n                        (\"index\", np.int64),\n                        (\"amplitude\", np.float32),\n                        (\"channel\", np.int32),\n                    ]\n                ),\n            ),\n        )\n\n        # Update detection stats\n        self._detection_stats = {\n            \"fs\": fs,\n            \"positive_threshold\": pos_threshold,\n            \"negative_threshold\": neg_threshold,\n            \"min_distance_samples\": min_distance_samples,\n            \"overlap_samples\": self._overlap_samples,\n        }\n\n        # Compute the result\n        try:\n            result_data = result.compute()\n        except Exception as e:\n            print(f\"Error computing peaks: {e}\")\n            # Return empty DataFrame if computation fails\n            return pl.DataFrame(\n                schema={\n                    \"index\": pl.Int64,\n                    \"amplitude\": pl.Float32,\n                    \"channel\": pl.Int32,\n                    \"time_sec\": pl.Float64,\n                }\n            )\n\n        # Return empty DataFrame if no peaks found\n        if len(result_data) == 0:\n            return pl.DataFrame(\n                schema={\n                    \"index\": pl.Int64,\n                    \"amplitude\": pl.Float32,\n                    \"channel\": pl.Int32,\n                    \"time_sec\": pl.Float64,\n                }\n            )\n\n        # Convert to Polars DataFrame\n        df = pl.from_numpy(result_data)\n\n        # Add time in seconds column\n        df = df.with_columns((pl.col(\"index\") / fs).alias(\"time_sec\"))\n\n        return df\n\n    @property\n    def overlap_samples(self) -&gt; int:\n        \"\"\"Return the number of samples needed for overlap\"\"\"\n        return self._overlap_samples or 10  # Default to 10 if not yet calculated\n\n    @property\n    def summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get a summary of the peak detector configuration\"\"\"\n        base_summary = super().summary\n        base_summary.update(\n            {\n                \"align_to_peak\": self.align_to_peak,\n                \"detect_positive\": self.detect_positive,\n                \"detect_negative\": self.detect_negative,\n                \"overlap_samples\": self._overlap_samples,\n            }\n        )\n        return base_summary\n</code></pre>"},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector-attributes","title":"Attributes","text":""},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector.overlap_samples","title":"<code>overlap_samples</code>  <code>property</code>","text":"<p>Return the number of samples needed for overlap</p>"},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector.summary","title":"<code>summary</code>  <code>property</code>","text":"<p>Get a summary of the peak detector configuration</p>"},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector-functions","title":"Functions","text":""},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector.__init__","title":"<code>__init__(threshold=4.0, threshold_mode='mad', polarity='negative', refractory_period=0.001, align_to_peak=True, noise_estimation_kwargs=None)</code>","text":"<p>Initialize the peak detector.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold value (interpretation depends on threshold_mode)</p> <code>4.0</code> <code>threshold_mode</code> <code>Literal['absolute', 'std', 'mad', 'rms']</code> <p>How to interpret the threshold \"absolute\": Use raw value \"std\": Multiple of signal standard deviation \"mad\": Multiple of median absolute deviation (more robust) \"rms\": Multiple of root mean square</p> <code>'mad'</code> <code>polarity</code> <code>Literal['positive', 'negative', 'both']</code> <p>Which peaks to detect \"positive\": Only detect positive peaks \"negative\": Only detect negative peaks (typical for extracellular spikes) \"both\": Detect both positive and negative peaks</p> <code>'negative'</code> <code>refractory_period</code> <code>float</code> <p>Minimum time between spikes in seconds</p> <code>0.001</code> <code>align_to_peak</code> <code>bool</code> <p>Whether to align detection to peak rather than threshold crossing</p> <code>True</code> <code>noise_estimation_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Parameters for noise estimation if threshold_mode is \"mad\"</p> <code>None</code> Source code in <code>src\\dspant\\neuroproc\\detection\\peak_detector.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 4.0,\n    threshold_mode: Literal[\"absolute\", \"std\", \"mad\", \"rms\"] = \"mad\",\n    polarity: Literal[\"positive\", \"negative\", \"both\"] = \"negative\",\n    refractory_period: float = 0.001,  # 1ms default refractory period\n    align_to_peak: bool = True,  # Align detection to peak rather than threshold crossing\n    noise_estimation_kwargs: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"\n    Initialize the peak detector.\n\n    Args:\n        threshold: Threshold value (interpretation depends on threshold_mode)\n        threshold_mode: How to interpret the threshold\n            \"absolute\": Use raw value\n            \"std\": Multiple of signal standard deviation\n            \"mad\": Multiple of median absolute deviation (more robust)\n            \"rms\": Multiple of root mean square\n        polarity: Which peaks to detect\n            \"positive\": Only detect positive peaks\n            \"negative\": Only detect negative peaks (typical for extracellular spikes)\n            \"both\": Detect both positive and negative peaks\n        refractory_period: Minimum time between spikes in seconds\n        align_to_peak: Whether to align detection to peak rather than threshold crossing\n        noise_estimation_kwargs: Parameters for noise estimation if threshold_mode is \"mad\"\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        threshold_mode=threshold_mode,\n        polarity=polarity,\n        refractory_period=refractory_period,\n    )\n\n    self.align_to_peak = align_to_peak\n\n    # Set up noise estimation if needed\n    self.noise_estimation_kwargs = noise_estimation_kwargs or {}\n\n    if self.threshold_mode == \"mad\":\n        self.noise_estimator = NoiseEstimationProcessor(\n            method=\"mad\", **self.noise_estimation_kwargs\n        )\n    else:\n        self.noise_estimator = None\n\n    # Overlap samples will be calculated dynamically based on fs and refractory_period\n    self._overlap_samples = None\n\n    # Initialize detection flags\n    self.detect_positive = polarity in [\"positive\", \"both\"]\n    self.detect_negative = polarity in [\"negative\", \"both\"]\n</code></pre>"},{"location":"neural-processor/api/#dspant.neuroproc.detection.PeakDetector.detect","title":"<code>detect(data, fs, **kwargs)</code>","text":"<p>Detect peaks in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Array</code> <p>Input dask array</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame with detection results containing: - index: Sample index of the peak - amplitude: Peak amplitude - channel: Channel number - time_sec: Time of the peak in seconds</p> Source code in <code>src\\dspant\\neuroproc\\detection\\peak_detector.py</code> <pre><code>def detect(self, data: da.Array, fs: float, **kwargs) -&gt; pl.DataFrame:\n    \"\"\"\n    Detect peaks in the input data.\n\n    Args:\n        data: Input dask array\n        fs: Sampling frequency in Hz\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        Polars DataFrame with detection results containing:\n            - index: Sample index of the peak\n            - amplitude: Peak amplitude\n            - channel: Channel number\n            - time_sec: Time of the peak in seconds\n    \"\"\"\n    # Ensure data is 2D\n    if data.ndim == 1:\n        data = data.reshape(-1, 1)\n\n    # Calculate minimum distance in samples\n    min_distance_samples = max(int(self.refractory_period * fs), 1)\n\n    # Set overlap samples based on refractory period and sampling rate\n    # Using 3x refractory period ensures we don't miss peaks at chunk boundaries\n    self._overlap_samples = max(min_distance_samples * 3, 10)\n\n    # Calculate adaptive thresholds if using MAD-based threshold\n    if self.threshold_mode == \"mad\" and self.noise_estimator is not None:\n        noise_levels = self.noise_estimator.process(data, fs=fs)\n\n        # Convert to numpy for easier handling\n        noise_levels = noise_levels.compute()\n\n        # For simplicity, use the mean noise level across channels for threshold\n        mean_noise = float(np.mean(noise_levels))\n        pos_threshold = self.threshold * mean_noise\n        neg_threshold = -self.threshold * mean_noise\n    else:\n        # Use compute_threshold from base class for other threshold types\n        thresholds = self.compute_threshold(data.compute(), fs)\n        # Use the mean threshold across channels\n        pos_threshold = float(np.mean(thresholds[\"positive\"]))\n        neg_threshold = float(np.mean(thresholds[\"negative\"]))\n\n    # Function to apply to each chunk with specified overlap\n    def detect_peaks_chunk(chunk, block_info=None):\n        # Get chunk offset for correct indexing in the full array\n        chunk_offset = 0\n        if (\n            block_info\n            and len(block_info) &gt; 0\n            and len(block_info[0][\"array-location\"]) &gt; 0\n        ):\n            try:\n                chunk_offset = block_info[0][\"array-location\"][0][0]\n            except (IndexError, KeyError):\n                chunk_offset = 0\n\n        # Ensure chunk is contiguous and in correct layout\n        chunk = np.ascontiguousarray(chunk)\n\n        # Use numba-accelerated implementation\n        peak_indices, peak_amplitudes, peak_channels = _detect_peaks_numba(\n            chunk,\n            pos_threshold,\n            neg_threshold,\n            self.detect_positive,\n            self.detect_negative,\n            min_distance_samples,\n        )\n\n        # Create empty result with correct structure when no peaks are found\n        if len(peak_indices) == 0:\n            # Return empty array but with the correct structured dtype\n            return np.array(\n                [],\n                dtype=[\n                    (\"index\", np.int64),\n                    (\"amplitude\", np.float32),\n                    (\"channel\", np.int32),\n                ],\n            )\n\n        # Add chunk offset to indices\n        peak_indices = peak_indices + chunk_offset\n\n        # Create structured array for output\n        result = np.zeros(\n            len(peak_indices),\n            dtype=[\n                (\"index\", np.int64),\n                (\"amplitude\", np.float32),\n                (\"channel\", np.int32),\n            ],\n        )\n\n        result[\"index\"] = peak_indices\n        result[\"amplitude\"] = peak_amplitudes\n        result[\"channel\"] = peak_channels\n\n        return result\n\n    # Apply detection to chunks with overlap\n    result = data.map_overlap(\n        detect_peaks_chunk,\n        depth={-2: self._overlap_samples},\n        boundary=\"reflect\",\n        dtype=np.dtype(\n            [\n                (\"index\", np.int64),\n                (\"amplitude\", np.float32),\n                (\"channel\", np.int32),\n            ]\n        ),\n        meta=np.array(\n            [],\n            dtype=np.dtype(\n                [\n                    (\"index\", np.int64),\n                    (\"amplitude\", np.float32),\n                    (\"channel\", np.int32),\n                ]\n            ),\n        ),\n    )\n\n    # Update detection stats\n    self._detection_stats = {\n        \"fs\": fs,\n        \"positive_threshold\": pos_threshold,\n        \"negative_threshold\": neg_threshold,\n        \"min_distance_samples\": min_distance_samples,\n        \"overlap_samples\": self._overlap_samples,\n    }\n\n    # Compute the result\n    try:\n        result_data = result.compute()\n    except Exception as e:\n        print(f\"Error computing peaks: {e}\")\n        # Return empty DataFrame if computation fails\n        return pl.DataFrame(\n            schema={\n                \"index\": pl.Int64,\n                \"amplitude\": pl.Float32,\n                \"channel\": pl.Int32,\n                \"time_sec\": pl.Float64,\n            }\n        )\n\n    # Return empty DataFrame if no peaks found\n    if len(result_data) == 0:\n        return pl.DataFrame(\n            schema={\n                \"index\": pl.Int64,\n                \"amplitude\": pl.Float32,\n                \"channel\": pl.Int32,\n                \"time_sec\": pl.Float64,\n            }\n        )\n\n    # Convert to Polars DataFrame\n    df = pl.from_numpy(result_data)\n\n    # Add time in seconds column\n    df = df.with_columns((pl.col(\"index\") / fs).alias(\"time_sec\"))\n\n    return df\n</code></pre>"},{"location":"neural-processor/api/#dspant.neuroproc.detection-functions","title":"Functions","text":""},{"location":"neural-processor/api/#dspant.neuroproc.detection.create_negative_peak_detector","title":"<code>create_negative_peak_detector(threshold=4.0, refractory_period=0.001)</code>","text":"<p>Create a detector optimized for negative spikes (common in extracellular recordings).</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>MAD threshold multiplier</p> <code>4.0</code> <code>refractory_period</code> <code>float</code> <p>Minimum time between spikes</p> <code>0.001</code> <p>Returns:</p> Type Description <code>PeakDetector</code> <p>Configured PeakDetector instance</p> Source code in <code>src\\dspant\\neuroproc\\detection\\peak_detector.py</code> <pre><code>def create_negative_peak_detector(\n    threshold: float = 4.0,\n    refractory_period: float = 0.001,\n) -&gt; PeakDetector:\n    \"\"\"\n    Create a detector optimized for negative spikes (common in extracellular recordings).\n\n    Args:\n        threshold: MAD threshold multiplier\n        refractory_period: Minimum time between spikes\n\n    Returns:\n        Configured PeakDetector instance\n    \"\"\"\n    return PeakDetector(\n        threshold=threshold,\n        threshold_mode=\"mad\",\n        polarity=\"negative\",\n        refractory_period=refractory_period,\n        align_to_peak=True,\n        noise_estimation_kwargs={\"relative_start\": 0.0, \"relative_stop\": 0.2},\n    )\n</code></pre>"},{"location":"neural-processor/api/#dspant.neuroproc.detection.create_positive_peak_detector","title":"<code>create_positive_peak_detector(threshold=4.0, refractory_period=0.001)</code>","text":"<p>Create a detector optimized for positive spikes.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>MAD threshold multiplier</p> <code>4.0</code> <code>refractory_period</code> <code>float</code> <p>Minimum time between spikes</p> <code>0.001</code> <p>Returns:</p> Type Description <code>PeakDetector</code> <p>Configured PeakDetector instance</p> Source code in <code>src\\dspant\\neuroproc\\detection\\peak_detector.py</code> <pre><code>def create_positive_peak_detector(\n    threshold: float = 4.0,\n    refractory_period: float = 0.001,\n) -&gt; PeakDetector:\n    \"\"\"\n    Create a detector optimized for positive spikes.\n\n    Args:\n        threshold: MAD threshold multiplier\n        refractory_period: Minimum time between spikes\n\n    Returns:\n        Configured PeakDetector instance\n    \"\"\"\n    return PeakDetector(\n        threshold=threshold,\n        threshold_mode=\"mad\",\n        polarity=\"positive\",\n        refractory_period=refractory_period,\n        align_to_peak=True,\n        noise_estimation_kwargs={\"relative_start\": 0.0, \"relative_stop\": 0.2},\n    )\n</code></pre>"},{"location":"neural-processor/api/#dspant.neuroproc.detection.create_bipolar_peak_detector","title":"<code>create_bipolar_peak_detector(threshold=4.0, refractory_period=0.001)</code>","text":"<p>Create a detector that finds both positive and negative peaks.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>MAD threshold multiplier</p> <code>4.0</code> <code>refractory_period</code> <code>float</code> <p>Minimum time between spikes</p> <code>0.001</code> <p>Returns:</p> Type Description <code>PeakDetector</code> <p>Configured PeakDetector instance</p> Source code in <code>src\\dspant\\neuroproc\\detection\\peak_detector.py</code> <pre><code>def create_bipolar_peak_detector(\n    threshold: float = 4.0,\n    refractory_period: float = 0.001,\n) -&gt; PeakDetector:\n    \"\"\"\n    Create a detector that finds both positive and negative peaks.\n\n    Args:\n        threshold: MAD threshold multiplier\n        refractory_period: Minimum time between spikes\n\n    Returns:\n        Configured PeakDetector instance\n    \"\"\"\n    return PeakDetector(\n        threshold=threshold,\n        threshold_mode=\"mad\",\n        polarity=\"both\",\n        refractory_period=refractory_period,\n        align_to_peak=True,\n        noise_estimation_kwargs={\"relative_start\": 0.0, \"relative_stop\": 0.2},\n    )\n</code></pre>"},{"location":"neural-processor/api/#extraction-module","title":"Extraction Module","text":"<p>Waveform extraction module for neural data analysis.</p> <p>This module provides functionality for extracting and aligning spike waveforms from continuous neural recordings, optimized for large datasets.</p> <p>Functions:</p> Name Description <code>extract_spike_waveforms</code> <p>Extract waveforms around specified spike times</p> <code>align_waveforms</code> <p>Align existing waveforms to their peaks</p> <code>batch_extract_waveforms</code> <p>Process large datasets in memory-efficient batches</p>"},{"location":"neural-processor/api/#dspant.neuroproc.extraction-functions","title":"Functions","text":""},{"location":"neural-processor/api/#dspant.neuroproc.extraction.extract_spike_waveforms","title":"<code>extract_spike_waveforms(data, spike_indices, pre_samples=10, post_samples=40, align_to_min=True, align_window=5, align_interp=False, max_jitter=3, use_numba=True, compute_result=False)</code>","text":"<p>Extract spike waveforms from Dask array and align them.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Array</code> <p>Raw data Dask array (samples x channels)</p> required <code>spike_indices</code> <code>Union[ndarray, DataFrame]</code> <p>Spike detection results (DataFrame or structured array)</p> required <code>pre_samples</code> <code>int</code> <p>Number of samples to take before the spike peak</p> <code>10</code> <code>post_samples</code> <code>int</code> <p>Number of samples to take after the spike peak</p> <code>40</code> <code>align_to_min</code> <code>bool</code> <p>Whether to align spikes to their minimum (negative peak)</p> <code>True</code> <code>align_window</code> <code>int</code> <p>Window around detected peak to search for alignment</p> <code>5</code> <code>align_interp</code> <code>bool</code> <p>Whether to use cubic interpolation for sub-sample alignment</p> <code>False</code> <code>max_jitter</code> <code>int</code> <p>Maximum allowed jitter for alignment</p> <code>3</code> <code>use_numba</code> <code>bool</code> <p>Whether to use Numba acceleration for alignment (when computing)</p> <code>True</code> <code>compute_result</code> <code>bool</code> <p>Whether to immediately compute Dask results into NumPy arrays</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Tuple of (waveforms, spike_times, metadata)</p> <code>ndarray</code> <ul> <li>waveforms: Dask array of aligned spike waveforms (spikes x samples)</li> </ul> <code>Dict</code> <ul> <li>spike_times: Array of spike times in samples</li> </ul> <code>Tuple[Array, ndarray, Dict]</code> <ul> <li>metadata: Dictionary with additional information</li> </ul> Source code in <code>src\\dspant\\neuroproc\\extraction\\waveform_extractor.py</code> <pre><code>def extract_spike_waveforms(\n    data: da.Array,\n    spike_indices: Union[np.ndarray, pl.DataFrame],\n    pre_samples: int = 10,\n    post_samples: int = 40,\n    align_to_min: bool = True,\n    align_window: int = 5,\n    align_interp: bool = False,\n    max_jitter: int = 3,\n    use_numba: bool = True,\n    compute_result: bool = False,\n) -&gt; Tuple[da.Array, np.ndarray, Dict]:\n    \"\"\"\n    Extract spike waveforms from Dask array and align them.\n\n    Args:\n        data: Raw data Dask array (samples x channels)\n        spike_indices: Spike detection results (DataFrame or structured array)\n        pre_samples: Number of samples to take before the spike peak\n        post_samples: Number of samples to take after the spike peak\n        align_to_min: Whether to align spikes to their minimum (negative peak)\n        align_window: Window around detected peak to search for alignment\n        align_interp: Whether to use cubic interpolation for sub-sample alignment\n        max_jitter: Maximum allowed jitter for alignment\n        use_numba: Whether to use Numba acceleration for alignment (when computing)\n        compute_result: Whether to immediately compute Dask results into NumPy arrays\n\n    Returns:\n        Tuple of (waveforms, spike_times, metadata)\n        - waveforms: Dask array of aligned spike waveforms (spikes x samples)\n        - spike_times: Array of spike times in samples\n        - metadata: Dictionary with additional information\n    \"\"\"\n    # Convert spike_indices to numpy arrays if it's a DataFrame\n    if isinstance(spike_indices, pl.DataFrame):\n        spike_times = spike_indices[\"index\"].to_numpy()\n        channels = (\n            spike_indices[\"channel\"].to_numpy()\n            if \"channel\" in spike_indices.columns\n            else np.zeros(len(spike_times), dtype=np.int32)\n        )\n        amplitudes = (\n            spike_indices[\"amplitude\"].to_numpy()\n            if \"amplitude\" in spike_indices.columns\n            else np.zeros(len(spike_times), dtype=np.float32)\n        )\n    else:\n        # Assume structured numpy array or plain array of indices\n        try:\n            spike_times = spike_indices[\"index\"]\n            channels = (\n                spike_indices[\"channel\"]\n                if \"channel\" in spike_indices.dtype.names\n                else np.zeros(len(spike_times), dtype=np.int32)\n            )\n            amplitudes = (\n                spike_indices[\"amplitude\"]\n                if \"amplitude\" in spike_indices.dtype.names\n                else np.zeros(len(spike_times), dtype=np.float32)\n            )\n        except (TypeError, IndexError, KeyError):\n            # Assume a plain array of indices\n            spike_times = np.asarray(spike_indices)\n            channels = np.zeros(len(spike_times), dtype=np.int32)\n            amplitudes = np.zeros(len(spike_times), dtype=np.float32)\n\n    # Get number of spikes and data shape\n    n_spikes = len(spike_times)\n    if n_spikes == 0:\n        # Return empty result\n        empty_result = da.empty(\n            (0, pre_samples + post_samples + 1, data.shape[1] if data.ndim &gt; 1 else 1),\n            chunks=(\n                1,\n                pre_samples + post_samples + 1,\n                data.shape[1] if data.ndim &gt; 1 else 1,\n            ),\n            dtype=data.dtype,\n        )\n        if compute_result:\n            empty_result = empty_result.compute()\n        return (empty_result, np.array([]), {\"n_spikes\": 0, \"channels\": np.array([])})\n\n    # Ensure data is 2D\n    if data.ndim == 1:\n        data = data.reshape(-1, 1)\n\n    n_samples, n_channels = data.shape\n    waveform_length = pre_samples + post_samples + 1\n\n    # Filter spike times to prevent errors\n    valid_mask = (spike_times &gt;= pre_samples) &amp; (spike_times &lt; n_samples - post_samples)\n    if not np.all(valid_mask):\n        spike_times = spike_times[valid_mask]\n        channels = channels[valid_mask]\n        amplitudes = amplitudes[valid_mask]\n        print(f\"Filtered {np.sum(~valid_mask)} invalid spike times\")\n\n    # Extract waveforms from Dask array\n    # The choice to use Numba for alignment is handled inside the function\n    return _extract_dask_waveforms(\n        data=data,\n        spike_times=spike_times,\n        channels=channels,\n        amplitudes=amplitudes,\n        pre_samples=pre_samples,\n        post_samples=post_samples,\n        align_to_min=align_to_min,\n        align_window=align_window,\n        align_interp=align_interp,\n        max_jitter=max_jitter,\n        use_numba=use_numba,\n        compute_result=compute_result,\n    )\n</code></pre>"},{"location":"neural-processor/api/#dspant.neuroproc.extraction.align_waveforms","title":"<code>align_waveforms(waveforms, align_to_min=True, align_window=5, max_jitter=3, use_numba=True, compute_result=False)</code>","text":"<p>Align spike waveforms to their peak. Supports both Dask arrays and NumPy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>waveforms</code> <code>Union[Array, ndarray]</code> <p>Array of spike waveforms (spikes x samples x channels)</p> required <code>align_to_min</code> <code>bool</code> <p>Whether to align to minimum (negative peak)</p> <code>True</code> <code>align_window</code> <code>int</code> <p>Window size to search for peak</p> <code>5</code> <code>max_jitter</code> <code>int</code> <p>Maximum allowed jitter in samples</p> <code>3</code> <code>use_numba</code> <code>bool</code> <p>Whether to use Numba acceleration (when using NumPy arrays)</p> <code>True</code> <code>compute_result</code> <code>bool</code> <p>Whether to immediately compute Dask results into NumPy arrays</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Array, ndarray]</code> <p>Array of aligned waveforms</p> Source code in <code>src\\dspant\\neuroproc\\extraction\\waveform_extractor.py</code> <pre><code>def align_waveforms(\n    waveforms: Union[da.Array, np.ndarray],\n    align_to_min: bool = True,\n    align_window: int = 5,\n    max_jitter: int = 3,\n    use_numba: bool = True,\n    compute_result: bool = False,\n) -&gt; Union[da.Array, np.ndarray]:\n    \"\"\"\n    Align spike waveforms to their peak.\n    Supports both Dask arrays and NumPy arrays.\n\n    Args:\n        waveforms: Array of spike waveforms (spikes x samples x channels)\n        align_to_min: Whether to align to minimum (negative peak)\n        align_window: Window size to search for peak\n        max_jitter: Maximum allowed jitter in samples\n        use_numba: Whether to use Numba acceleration (when using NumPy arrays)\n        compute_result: Whether to immediately compute Dask results into NumPy arrays\n\n    Returns:\n        Array of aligned waveforms\n    \"\"\"\n    # Handle NumPy array input\n    if isinstance(waveforms, np.ndarray):\n        n_spikes, n_samples, n_channels = waveforms.shape\n\n        if n_spikes == 0:\n            return waveforms\n\n        # Center sample index\n        pre_samples = n_samples // 2\n\n        # Determine polarities for each waveform\n        polarities = np.array(\n            [np.min(waveforms[i, :, 0]) &lt; 0 for i in range(n_spikes)], dtype=np.bool_\n        )\n\n        # Use Numba acceleration if requested\n        if use_numba:\n            return _align_multiple_waveforms(\n                waveforms, pre_samples, align_window, max_jitter, polarities\n            )\n        else:\n            # Plain NumPy alignment\n            aligned_waveforms = np.zeros_like(waveforms)\n            for i, is_negative in enumerate(polarities):\n                # Search for peak within alignment window\n                search_start = max(0, pre_samples - align_window)\n                search_end = min(n_samples, pre_samples + align_window + 1)\n\n                # Find peak position\n                if is_negative:\n                    peak_idx = (\n                        np.argmin(waveforms[i, search_start:search_end, 0])\n                        + search_start\n                    )\n                else:\n                    peak_idx = (\n                        np.argmax(waveforms[i, search_start:search_end, 0])\n                        + search_start\n                    )\n\n                # Calculate shift\n                shift = pre_samples - peak_idx\n\n                # Apply shift if within limits\n                if abs(shift) &lt;= max_jitter:\n                    if shift &gt; 0:\n                        # Shift right\n                        aligned_waveforms[i, shift:, :] = waveforms[i, :-shift, :]\n                    elif shift &lt; 0:\n                        # Shift left\n                        aligned_waveforms[i, :shift, :] = waveforms[i, -shift:, :]\n                    else:\n                        # No shift needed\n                        aligned_waveforms[i] = waveforms[i]\n                else:\n                    # Shift too large, keep original\n                    aligned_waveforms[i] = waveforms[i]\n\n            return aligned_waveforms\n\n    # Handle Dask array input\n    n_spikes, n_samples, n_channels = waveforms.shape\n\n    if n_spikes == 0:\n        if compute_result:\n            return waveforms.compute()\n        return waveforms\n\n    # Center sample index\n    pre_samples = n_samples // 2\n\n    # If we're computing the result, convert to NumPy and use Numba acceleration\n    if compute_result:\n        waveforms_np = waveforms.compute()\n        polarities = np.array(\n            [np.min(waveforms_np[i, :, 0]) &lt; 0 for i in range(n_spikes)], dtype=np.bool_\n        )\n\n        if use_numba:\n            return _align_multiple_waveforms(\n                waveforms_np, pre_samples, align_window, max_jitter, polarities\n            )\n        else:\n            # Use the NumPy implementation above\n            return align_waveforms(\n                waveforms_np, align_to_min, align_window, max_jitter, use_numba=False\n            )\n\n    # For Dask without computing, we need to process each waveform separately\n    aligned_chunks = []\n\n    # Process each waveform\n    for i in range(n_spikes):\n        # Compute the first channel for determining polarity\n        first_channel = waveforms[i, :, 0].compute()\n        is_negative = np.min(first_channel) &lt; 0\n\n        # Define search window\n        search_start = max(0, pre_samples - align_window)\n        search_end = min(n_samples, pre_samples + align_window + 1)\n\n        # Compute search window for peak finding\n        search_window = waveforms[i, search_start:search_end, 0].compute()\n\n        # Find peak position\n        if is_negative:\n            peak_idx = np.argmin(search_window) + search_start\n        else:\n            peak_idx = np.argmax(search_window) + search_start\n\n        # Calculate shift\n        shift = pre_samples - peak_idx\n\n        # Apply shift if within jitter limits\n        if abs(shift) &lt;= max_jitter:\n            if shift != 0:\n                # Create shifted version\n                if shift &gt; 0:\n                    # Shift right\n                    aligned_chunk = da.pad(\n                        waveforms[i, :-shift]\n                        if shift &lt; n_samples\n                        else waveforms[i, 0:0],\n                        ((shift, 0), (0, 0)),\n                        mode=\"constant\",\n                    )\n                else:\n                    # Shift left\n                    aligned_chunk = da.pad(\n                        waveforms[i, -shift:]\n                        if -shift &lt; n_samples\n                        else waveforms[i, 0:0],\n                        ((0, -shift), (0, 0)),\n                        mode=\"constant\",\n                    )\n            else:\n                # No shift needed\n                aligned_chunk = waveforms[i]\n        else:\n            # Shift too large, keep original\n            aligned_chunk = waveforms[i]\n\n        aligned_chunks.append(aligned_chunk)\n\n    # Stack aligned waveforms into a Dask array\n    aligned_waveforms = da.stack(aligned_chunks)\n\n    return aligned_waveforms\n</code></pre>"},{"location":"neural-processor/overview/","title":"Neural Processing Overview","text":"<p>The <code>neuroproc</code> module is a specialized collection of tools for neural data analysis, focusing particularly on spike detection, waveform extraction, spike sorting, and quality assessment. This module builds on dspant's core architecture to provide optimized algorithms for neural data analysis workflows.</p>"},{"location":"neural-processor/overview/#module-structure","title":"Module Structure","text":"<p>The neural processing module is organized into several submodules:</p> <ul> <li>Detection: Algorithms for identifying neural action potentials (spikes)</li> <li>Extraction: Tools for extracting waveforms around detected spikes</li> <li>Visualization: Plotting and visualization tools for neural data and analysis results</li> <li>Sorters: Spike sorting algorithms for clustering neural units</li> <li>Metrics: Quality metrics and validation tools for spike sorting results</li> <li>Utils: Utility functions supporting various neural processing tasks</li> </ul>"},{"location":"neural-processor/overview/#key-features","title":"Key Features","text":""},{"location":"neural-processor/overview/#high-performance-processing","title":"High-Performance Processing","text":"<p>Neural processing algorithms are optimized for performance using:</p> <ul> <li>Numba: Just-in-time compilation for CPU-intensive operations</li> <li>Dask: Parallel and distributed computing for large datasets</li> <li>Chunked Processing: Processing data in manageable chunks to handle large recordings</li> </ul>"},{"location":"neural-processor/overview/#comprehensive-workflow","title":"Comprehensive Workflow","text":"<p>The module supports a complete neural data analysis workflow:</p> <ol> <li>Spike Detection: Identify spike events in continuous recordings</li> <li>Waveform Extraction: Extract and align spike waveforms</li> <li>Feature Extraction: Compute features for clustering</li> <li>Spike Sorting: Cluster waveforms into putative neural units</li> <li>Quality Assessment: Evaluate the quality of sorted units</li> <li>Visualization: Visualize results at each stage of the workflow</li> </ol>"},{"location":"neural-processor/overview/#modularity-and-extensibility","title":"Modularity and Extensibility","text":"<p>Each component follows dspant's modular design principles:</p> <ul> <li>Processors implement the <code>BaseProcessor</code> interface for pipeline integration</li> <li>Factory functions provide simplified interfaces for common use cases</li> <li>Clear separation between algorithms and data structures</li> </ul>"},{"location":"neural-processor/overview/#common-workflow","title":"Common Workflow","text":"<p>A typical neural data analysis workflow with dspant might look like:</p> <pre><code># Import components\nfrom dspant.nodes import StreamNode\nfrom dspant.engine import create_processing_node\nfrom dspant.neuroproc.detection import create_negative_peak_detector\nfrom dspant.neuroproc.extraction import extract_spike_waveforms\nfrom dspant.neuroproc.sorters import create_numba_pca_kmeans\nfrom dspant.neuroproc.metrics import compute_quality_metrics\nfrom dspant.neuroproc.visualization import plot_spike_events, plot_waveform_clusters\n\n# 1. Load data\nnode = StreamNode(\"path/to/data.ant\").load_data()\n\n# 2. Set up processing pipeline with spike detector\nproc_node = create_processing_node(node)\ndetector = create_negative_peak_detector(threshold=4.0)\nproc_node.add_processor(detector, group=\"detection\")\n\n# 3. Detect spikes\nspikes_df = proc_node.process()\n\n# 4. Extract waveforms\nwaveforms, spike_times, metadata = extract_spike_waveforms(\n    node.data, spikes_df, pre_samples=10, post_samples=30\n)\n\n# 5. Sort spikes with PCA-KMeans\nsorter = create_numba_pca_kmeans(n_clusters=3, n_components=10)\ncluster_labels = sorter.process(waveforms)\n\n# 6. Visualize results\ncluster_fig = sorter.plot_clusters(waveforms=waveforms)\nspike_fig, raster_fig = plot_spike_events(\n    node.data, spikes_df, node.fs, cluster_column=\"cluster\"\n)\n\n# 7. Compute quality metrics\nfrom dspant.neuroproc.utils import prepare_spike_data_for_metrics\nmetrics_data = prepare_spike_data_for_metrics(\n    spike_times=spike_times,\n    cluster_labels=cluster_labels,\n    sampling_frequency=node.fs\n)\nmetrics = compute_quality_metrics(**metrics_data)\n</code></pre>"},{"location":"neural-processor/overview/#submodule-details","title":"Submodule Details","text":""},{"location":"neural-processor/overview/#detection","title":"Detection","text":"<p>The detection submodule provides spike detection algorithms:</p> <ul> <li>Threshold-based detection: Simple threshold crossing detection</li> <li>Peak detection: Identify local extrema (peaks) with thresholding</li> <li>Template matching: Detect spikes based on template similarity</li> </ul>"},{"location":"neural-processor/overview/#extraction","title":"Extraction","text":"<p>The extraction submodule handles spike waveform extraction:</p> <ul> <li>Waveform extraction: Extract time windows around detected spikes</li> <li>Waveform alignment: Align spikes to peaks for better clustering</li> <li>Batch processing: Efficiently process large datasets</li> </ul>"},{"location":"neural-processor/overview/#visualization","title":"Visualization","text":"<p>The visualization submodule offers specialized plotting functions:</p> <ul> <li>Spike visualization: Display detected spikes overlaid on raw data</li> <li>Raster plots: Show spike timing across channels or units</li> <li>Cluster visualization: Visualize spike sorting results with PCA projections</li> <li>Quality metrics visualization: Plot unit quality metrics</li> </ul>"},{"location":"neural-processor/overview/#sorters","title":"Sorters","text":"<p>The sorters submodule implements spike sorting algorithms:</p> <ul> <li>PCA-KMeans: Dimensionality reduction with PCA followed by KMeans clustering</li> <li>Numba-accelerated versions: High-performance implementations</li> </ul>"},{"location":"neural-processor/overview/#metrics","title":"Metrics","text":"<p>The metrics submodule provides quality assessment tools:</p> <ul> <li>ISI violations: Measure refractory period violations</li> <li>Presence ratio: Assess unit stability over time</li> <li>Amplitude cutoff: Estimate fraction of missing spikes</li> <li>Signal-to-noise ratio: Measure unit isolation quality</li> </ul>"},{"location":"neural-processor/overview/#utils","title":"Utils","text":"<p>The utils submodule contains supporting utilities:</p> <ul> <li>Spike utilities: Helper functions for spike data handling</li> <li>Template utilities: Functions for working with templates</li> <li>PHY exporter: Tools for exporting to the PHY template-GUI format</li> </ul>"}]}